import pandas as pd

# Step 1: Read the large CSV file in chunks
chunk_size = 10000
start_date = '2023-01-01'
end_date = '2023-12-31'
processed_chunks = []

for chunk in pd.read_csv('your_large_file.csv', dtype=str, chunksize=chunk_size):
    # Step 2: Convert the 'datetime_column' to datetime format
    chunk['datetime_column'] = pd.to_datetime(chunk['datetime_column'], errors='coerce')

    # Step 3: Calculate max values per key (you can use groupby and agg)
    max_values = chunk.groupby('key').agg(
        max_datetime=('datetime_column', 'max'),
        max_column1=('column1', 'max'),
        max_column2=('column2', 'max')
    ).reset_index()

    # Step 4: Filter max values by the date range
    filtered_max_values = max_values[(max_values['max_datetime'] >= start_date) & 
                                     (max_values['max_datetime'] <= end_date)]

    # Step 5: Merge (inner join) with the original chunk
    merged_chunk = pd.merge(chunk, filtered_max_values, 
                            how='inner', 
                            on=['key', 'datetime_column'])

    # Collect the processed chunks
    processed_chunks.append(merged_chunk)

# Step 6: Concatenate all processed chunks into one DataFrame
final_df = pd.concat(processed_chunks, ignore_index=True)

# Step 7: Output the result to a CSV file
final_df.to_csv('filtered_output.csv', index=False)

print("Process complete. The result is saved in 'filtered_output.csv'.")