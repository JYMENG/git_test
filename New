import os
import pandas as pd
from bs4 import BeautifulSoup

# Main folder path
root_dir = '/path/to/your/folder'

# Collect all HTML/HTM file paths, sorted by descending subfolder path
html_files = []
for dirpath, dirnames, filenames in os.walk(root_dir):
    for file in filenames:
        if file.lower().endswith(('.htm', '.html')):
            html_files.append(os.path.join(dirpath, file))

# Sort by descending folder path
html_files = sorted(html_files, key=lambda x: os.path.dirname(x), reverse=True)

# Parse and collect DataFrames
all_rows = []
first_columns = None
header_written = False

for file_path in html_files:
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        soup = BeautifulSoup(f, 'html.parser')
        tables = pd.read_html(str(soup))
        if tables:
            df = tables[0]
            if df.shape[1] < 2:
                continue  # Skip if dropping 1st col leaves empty or meaningless table
            df = df.iloc[:, 1:]  # Drop first column

            if not header_written:
                first_columns = df.columns
                header_written = True
            else:
                df.columns = first_columns  # Force consistent headers

            all_rows.append(df)

# Combine all, remove duplicates
combined = pd.concat(all_rows, ignore_index=True).drop_duplicates()

# Write to CSV
combined.to_csv('combined_output.csv', index=False)