import os
import pandas as pd
import sqlite3

# --- CONFIGURATION ---
FOLDER1 = "path_to_folder1"  # Update with actual path
FOLDER2 = "path_to_folder2"  # Update with actual path
USER_FILE = "path_to_user_file.xlsx"  # Update with actual path
DB_PATH = "merged_data.db"
BATCH_SIZE = 50000  # Number of rows to process per batch
DEBUG = True  # Set to False to disable debugging logs

# --- SQLITE CONNECTION ---
conn = sqlite3.connect(DB_PATH)
cursor = conn.cursor()


def log(message):
    """Prints debug messages if enabled."""
    if DEBUG:
        print(message)


# --- FUNCTION TO PROCESS LARGE EXCEL FILES WITHOUT CHUNKSIZE ---
def process_excel(folder, table_name):
    """Reads large Excel files sheet-by-sheet in batches and inserts directly into SQLite."""
    log(f"Processing folder: {folder}")

    cursor.execute(f"DROP TABLE IF EXISTS {table_name}")  # Clean previous data

    for file in os.listdir(folder):
        if file.endswith(".xlsx"):
            file_path = os.path.join(folder, file)
            log(f"  - Reading file: {file}")

            xl = pd.ExcelFile(file_path, engine="openpyxl")

            sheets = [s for s in xl.sheet_names if "SQL" not in s]  # Ignore 'SQL' sheets
            if not sheets:
                log(f"    * No valid sheets found in {file}, skipping.")
                continue

            for sheet_name in sheets:
                log(f"    * Processing sheet: {sheet_name}")
                
                df_iter = pd.read_excel(file_path, sheet_name=sheet_name, engine="openpyxl", iterator=True)
                first_chunk = next(df_iter)  # Read first chunk to get column headers
                
                first_chunk.columns = first_chunk.iloc[0]  # Set first row as header
                first_chunk = first_chunk[1:]  # Remove header row

                first_chunk.to_sql(table_name, conn, if_exists="append", index=False)
                log(f"      * Inserted {len(first_chunk)} rows from {sheet_name}.")
                
                batch = []
                for row in df_iter:
                    batch.append(row)
                    if len(batch) >= BATCH_SIZE:
                        df_batch = pd.DataFrame(batch)
                        df_batch.to_sql(table_name, conn, if_exists="append", index=False)
                        log(f"      * Inserted {len(df_batch)} rows from {sheet_name}.")
                        batch.clear()

                if batch:
                    df_batch = pd.DataFrame(batch)
                    df_batch.to_sql(table_name, conn, if_exists="append", index=False)
                    log(f"      * Inserted remaining {len(df_batch)} rows from {sheet_name}.")

    log(f"Completed processing for {folder}.")


# --- PROCESS BOTH FOLDERS INTO SQLITE ---
process_excel(FOLDER1, "folder1_merged")
process_excel(FOLDER2, "folder2_merged")

# --- CREATE INDEXES FOR FAST JOINS ---
log("Creating indexes on key columns...")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_f1_key ON folder1_merged(KeyColumn1)")
cursor.execute("CREATE INDEX IF NOT EXISTS idx_f2_key ON folder2_merged(KeyColumn2)")
conn.commit()
log("Indexes created successfully.")

# --- PERFORM LEFT JOIN ---
log("Performing LEFT JOIN on merged tables...")
cursor.execute("DROP TABLE IF EXISTS merged_results")
query = """
CREATE TABLE merged_results AS
SELECT f1.*, f2.*
FROM folder1_merged AS f1
LEFT JOIN folder2_merged AS f2
ON f1.KeyColumn1 = f2.KeyColumn2
"""
cursor.execute(query)
conn.commit()
log("LEFT JOIN completed successfully.")

# --- LOAD USER FILTER LIST ---
log(f"Loading user filter list from {USER_FILE}...")
df_users = pd.read_excel(USER_FILE, usecols=["UserID"])
user_ids = tuple(df_users["UserID"].tolist())  # Convert to tuple for SQL filtering
log(f"Loaded {len(user_ids)} user IDs.")

# --- FILTER & SAVE RESULTS IN BATCHES ---
log("Applying user filter and exporting results in chunks...")
OUTPUT_FILE = "filtered_results.csv"
chunk_offset = 0

with open(OUTPUT_FILE, "w") as f_out:
    while True:
        filtered_query = f"""
        SELECT * FROM merged_results
        WHERE UserID IN {user_ids}
        LIMIT {BATCH_SIZE} OFFSET {chunk_offset}
        """
        df_chunk = pd.read_sql(filtered_query, conn)

        if df_chunk.empty:
            break  # Stop if no more data

        df_chunk.to_csv(f_out, mode="a", header=(chunk_offset == 0), index=False)
        log(f"  * Saved {len(df_chunk)} rows to {OUTPUT_FILE} (offset {chunk_offset}).")
        chunk_offset += BATCH_SIZE

log("Filtering & export completed!")

# --- CLEANUP ---
conn.close()
log("SQLite connection closed.")