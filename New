import os
import pandas as pd
import sqlite3

# Step 1: Define paths and variables
folder_path = 'path_to_your_folder'  # Replace with your folder path
output_file = 'final_output.csv'  # Output file for the joined result
chunk_size = 10000  # Adjust depending on memory

# Columns to calculate max for and join on
columns_to_max = ['ColumnA', 'ColumnB']  # Replace with your column names
join_column = 'ColumnA'  # Replace with the column to join on

# Step 2: Initialize SQLite database
conn = sqlite3.connect(':memory:')  # Use an in-memory database for performance
cursor = conn.cursor()

# Step 3: Read and insert all files into SQLite
for file in os.listdir(folder_path):
    if file.endswith('.csv'):
        file_path = os.path.join(folder_path, file)
        print(f"Processing file: {file_path}")
        
        # Read the file in chunks
        for chunk in pd.read_csv(file_path, dtype=str, chunksize=chunk_size):
            # Insert chunk into SQLite
            chunk.to_sql('original_data', conn, if_exists='append', index=False)

# Step 4: Calculate max for specified columns grouped by ID
columns_to_max_sql = ', '.join([f'MAX({col}) AS max_{col}' for col in columns_to_max])
query_max = f"""
    CREATE TEMP TABLE max_values AS
    SELECT ID, {columns_to_max_sql}
    FROM original_data
    GROUP BY ID;
"""
cursor.execute(query_max)

# Step 5: Inner join the max values with the original data
query_join = f"""
    CREATE TEMP TABLE joined_data AS
    SELECT o.*
    FROM original_data o
    INNER JOIN max_values m
    ON o.ID = m.ID AND o.{join_column} = m.max_{join_column};
"""
cursor.execute(query_join)

# Step 6: Export the joined data to a CSV file
output_query = "SELECT * FROM joined_data"
joined_df = pd.read_sql_query(output_query, conn)
joined_df.to_csv(output_file, index=False)

# Step 7: Clean up
conn.close()
print(f"Processing complete. Results saved to {output_file}")