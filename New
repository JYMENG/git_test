import os
import pandas as pd
from bs4 import BeautifulSoup

# Main folder path
root_dir = '/path/to/your/folder'

# Collect all HTML/HTM file paths, sorted by descending subfolder path
html_files = []
for dirpath, dirnames, filenames in os.walk(root_dir):
    for file in filenames:
        if file.lower().endswith(('.htm', '.html')):
            html_files.append(os.path.join(dirpath, file))

# Sort by descending folder path
html_files = sorted(html_files, key=lambda x: os.path.dirname(x), reverse=True)

# Parse and collect DataFrames
all_rows = []
header_written = False
first_columns = None

for idx, file_path in enumerate(html_files):
    with open(file_path, 'r', encoding='utf-8', errors='ignore') as f:
        soup = BeautifulSoup(f, 'html.parser')
        tables = pd.read_html(str(soup))
        if tables:
            df = tables[0]
            if not header_written:
                first_columns = df.columns
                header_written = True
            else:
                # Force columns to match first header
                df.columns = first_columns
            all_rows.append(df)

# Combine all data, drop duplicates
combined = pd.concat(all_rows, ignore_index=True).drop_duplicates()

# Output
combined.to_csv('combined_output.csv', index=False)