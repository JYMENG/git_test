import pandas as pd

input_file = 'input.csv'
output_file = 'output_unique_col1.csv'

# Columns to sort by (update these)
sort_columns = ['col1', 'col2', 'col3']

# Keep track of seen 'col1' values
seen_col1 = set()
deduped_rows = []

# Process file in chunks
for chunk in pd.read_csv(input_file, chunksize=100_000):
    # Sort chunk
    chunk = chunk.sort_values(by=sort_columns)

    # Drop duplicates within chunk
    for _, row in chunk.iterrows():
        key = row['col1']
        if key not in seen_col1:
            seen_col1.add(key)
            deduped_rows.append(row)

# Final DataFrame and output
result_df = pd.DataFrame(deduped_rows)
result_df.to_csv(output_file, index=False)