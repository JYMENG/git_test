import pandas as pd
import os
import re

# File paths
merged_file_path = 'merged_output.csv'  # The merged dataset
output_dir = 'output_files'  # Directory to save the split files

# Ensure the output directory exists
os.makedirs(output_dir, exist_ok=True)

# Define chunk size for processing large files
chunksize = 50000  # Number of rows per chunk

# Process the merged file in chunks
for chunk in pd.read_csv(merged_file_path, chunksize=chunksize, encoding='ISO-8859-1'):
    # Split the target column (e.g., 'split_column') by delimiter '!' and extract the 4th item
    chunk['split_item'] = chunk['split_column'].str.split('!').str[3]
    
    # Handle cases where the 4th item is missing: fall back to the 3rd item
    chunk['split_item'] = chunk['split_item'].fillna(chunk['split_column'].str.split('!').str[2])

    # Retain only alphabetic characters in the extracted item
    chunk['split_item_cleaned'] = chunk['split_item'].apply(lambda x: ''.join(re.findall('[a-zA-Z]+', str(x))) if pd.notnull(x) else 'unknown')

    # Split the chunk into separate files based on the cleaned split item
    for name, group in chunk.groupby('split_item_cleaned'):
        output_file = os.path.join(output_dir, f"{name}.csv")
        # Append rows to the file if it already exists
        group.to_csv(output_file, mode='a', index=False, header=not os.path.exists(output_file))