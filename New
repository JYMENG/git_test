import pandas as pd

# Specify your input and output file paths
input_file = 'your_large_file.csv'  # Replace with your file path
output_file = 'flattened_output.csv'  # Output file

# Define the columns you want to keep in the final output
keep_columns = ['Column1', 'Column2', 'Column3']  # Replace with actual column names
flatten_columns = ['User', 'Time', 'Status']  # Columns to flatten

# Initialize an empty DataFrame to accumulate results
processed_chunks = []

# Read the file in chunks
chunksize = 50000  # Adjust chunk size based on your memory capacity
for chunk in pd.read_csv(input_file, chunksize=chunksize):
    # Sort by ID and Time columns
    chunk_sorted = chunk.sort_values(by=['ID', 'Time'])
    
    # Flatten User, Time, and Status columns based on ID
    chunk_grouped = chunk_sorted.groupby('ID').agg({
        'User': lambda x: ', '.join(x),
        'Time': lambda x: ', '.join(x),
        'Status': lambda x: ', '.join(x),
        **{col: 'first' for col in keep_columns}  # Keep first for the specified columns
    }).reset_index()
    
    # Append to the list of processed chunks
    processed_chunks.append(chunk_grouped)

# Concatenate all chunks into a single DataFrame
final_result = pd.concat(processed_chunks, ignore_index=True)

# Write the final result to a new CSV file
final_result.to_csv(output_file, index=False)

print("Processing complete. Output saved to:", output_file)