import os
import pandas as pd

folder_path = '/your/folder/path'
columns_to_use = ['col1', 'col2']
seen = set()
deduped_rows = []

files = sorted([
    f for f in os.listdir(folder_path)
    if f.lower().endswith('.csv')
])

for filename in files:
    full_path = os.path.join(folder_path, filename)
    try:
        for chunk in pd.read_csv(full_path, usecols=columns_to_use, chunksize=100_000):
            # Drop duplicates within chunk first
            chunk = chunk.drop_duplicates(subset=columns_to_use, keep='first')
            
            # Filter out already seen rows
            for _, row in chunk.iterrows():
                key = tuple(row[col] for col in columns_to_use)
                if key not in seen:
                    seen.add(key)
                    deduped_rows.append(row)
    except Exception as e:
        print(f"Error reading {filename}: {e}")

# Convert to final DataFrame
final_df = pd.DataFrame(deduped_rows, columns=columns_to_use)
print(final_df)